{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I uploaded a sample news story about American Express. Let's do some text analysis on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amex_news.txt\") as file:\n",
    "    amexnews=file.read()\n",
    "amexnews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "# Stopwords are words like a, an, the\n",
    "stopwords= set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.union(set(string.punctuation))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample='This is a sample sentence, which will illustrate how tokenization and stop words filtration works. This is another sentence. Profits went up by 3.4% this quarter. We will tokenize all this. And more.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplewords=nltk.tokenize.word_tokenize(sample)\n",
    "samplewords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of sentences in our sample string\n",
    "len(nltk.tokenize.sent_tokenize(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstopwords=[]\n",
    "for word in samplewords:\n",
    "    if not(word.lower() in stopwords):\n",
    "        nonstopwords.append(word)\n",
    "nonstopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstop_words= [word.lower() for word in samplewords if not(word.lower() in stopwords)]\n",
    "nonstop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say you want to count the frequency of words in a document\n",
    "- You probably want to treat different cases of the same word as the same (for example: words like \"dogs\" and \"dog\" should probably be thought of as one \"word\" for the purpose of word frequencies\n",
    "- You can do this with \"stemming\" or \"lemmatizing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words\n",
    "lemma = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma.lemmatize('dead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see Lemmatizing words is not perfect as \"died\" did not get converted to \"die\". There are advanced techniques that require giving the Lemmatizer the part-of-speech of the word to make for better lemmatizing. We will ignore this here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. convert text to lowercase\n",
    "2. tokenize into words\n",
    "3. remove stopwords and non-ABC characters\n",
    "4. lemmatize those stopwords\n",
    "'''\n",
    "amexwords=nltk.tokenize.word_tokenize(amexnews.lower())\n",
    "amex_ns_words=[word for word in amexwords if (word not in stopwords) and word.isalpha()]\n",
    "amex_lemma_words=[lemma.lemmatize(word) for word in amex_ns_words]\n",
    "amex_lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "amex_freq=nltk.FreqDist(amex_lemma_words)\n",
    "amex_freq.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a built-in sentiment score for each sentence in our Amex article\n",
    "sentiment=SentimentIntensityAnalyzer()\n",
    "amex_sentences=nltk.tokenize.sent_tokenize(amexnews)\n",
    "#s=sentiment.polarity_scores(amex_sentences[0])\n",
    "\n",
    "comp_scores=[] #store compound scores\n",
    "netpos_scores=[] #store pos - neg\n",
    "for s in amex_sentences:\n",
    "    comp_scores.append(sentiment.polarity_scores(s)['compound'])\n",
    "    pos=sentiment.polarity_scores(s)['pos']\n",
    "    neg=sentiment.polarity_scores(s)['neg']\n",
    "    netpos_scores.append(pos-neg)\n",
    "avgcomp=np.array(comp_scores).mean()\n",
    "avgnet=np.array(netpos_scores).mean()\n",
    "print(\"Average compound score is: \", avgcomp, \" Avg net pos is: \", avgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "- We discard the structures like paragraphs, sentences etc\n",
    "- We only count how often each word appears\n",
    "- Tokenization\n",
    "    - Split the document into words\n",
    "- Vocabulary Build\n",
    "    - Build a vocabulary of words that appear in all documents\n",
    "- Encoding\n",
    "    - For each document count the number of times a word occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the word frequencies of Apple's 10K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Apple 10-K 2017.txt') as file:\n",
    "    aapltext=file.read().lower()\n",
    "souptext=BeautifulSoup(aapltext).text #removes all the html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert to lower case\n",
    "2. Tokenize into words\n",
    "3. Remove stop words\n",
    "4. Remove all non \"ABC\" characters\n",
    "5. Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaplwords=nltk.tokenize.word_tokenize(souptext.lower())\n",
    "aapl_ns_words=[word for word in aaplwords if (word not in stopwords) and word.isalpha()]\n",
    "aapl_lemma_words=[lemma.lemmatize(word) for word in aapl_ns_words]\n",
    "aapl_lemma_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the LM dictionaries to calculate the percent of positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LM_Positive.txt') as file:\n",
    "    LM_Positive=file.read().lower()\n",
    "with open('LM_Negative.txt') as file:\n",
    "    LM_Negative=file.read().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_bag=nltk.tokenize.word_tokenize(LM_Positive)\n",
    "negative_bag=nltk.tokenize.word_tokenize(LM_Negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep positive words in our AAPL 10k\n",
    "aapl_poswords=[word for word in aapl_lemma_words if word in positive_bag]\n",
    "\n",
    "#Keep negative words in our AAPL 10k\n",
    "aapl_negwords=[word for word in aapl_lemma_words if word in negative_bag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Tone\n",
    "- Defined as %Positive - %Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pctpos=len(aapl_poswords)/len(aapl_lemma_words)\n",
    "pctneg=len(aapl_negwords)/len(aapl_lemma_words)\n",
    "tone=pctpos-pctneg\n",
    "tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_negwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate FOG Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "d=cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['economy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['photograph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['finance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['vineet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function to count the number of syllables per word using the stress markers from the CMU dictionary\n",
    "- For now, just take the first pronounciation (if there are multiple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numsyllables(word, d):\n",
    "    #receives a string \"word\"\n",
    "    #receives a dictionary \"d\"\n",
    "    #return the number of syllables in that word\n",
    "    try:\n",
    "        return len([y for y in d[word][0] if y[-1].isdigit()])\n",
    "    except:\n",
    "        return -999\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsyllables('tomato', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate percent of complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FOGindex(doctext, d):\n",
    "    #receives a long string of the document text\n",
    "    #receives a CMUdictionary of number of syllables\n",
    "    #returns FOG Index = .4*(avg number of words per sent + % complex words)\n",
    "    \n",
    "    numsentences=len(nltk.tokenize.sent_tokenize(doctext))\n",
    "    \n",
    "    #Total number of words\n",
    "    docwords=nltk.tokenize.word_tokenize(doctext.lower())\n",
    "    ns_words=[word for word in docwords if (word not in stopwords) and word.isalpha()]\n",
    "    lemma_words=[lemma.lemmatize(word) for word in ns_words]\n",
    "    totwords=len(lemma_words)\n",
    "    \n",
    "    #Total number of complex words\n",
    "    complex_words=[word for word in lemma_words if numsyllables(word, d)>2]\n",
    "    num_complex=len(complex_words)\n",
    "    \n",
    "    FOG=.4*((totwords/numsentences)+(num_complex/totwords))\n",
    "    return FOG, totwords, numsentences, num_complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOGindex(souptext,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOGindex(amexnews,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
